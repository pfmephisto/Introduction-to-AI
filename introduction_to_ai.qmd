---
title: "Introduction to AI"
subtitle: "aka. Machine learning"
author: "Povl Filip Sonne-Frederiksen"
format:
  revealjs:
    logo: images/LINK_logo_Black.png

    theme: 
        - simple
        - custom.scss

---

# Artificial Intelligence{background-color="black" background-image="/images/Metropolis-croped.jpg"}

Artificial Inteligence is a from of Machien Learning.

::: footer
Metropolise (1927), Maschinenmensch
:::


## What is Machine Learning? {background-image="/images/Starwars C-3PO.png"}

::: footer
Starwars I Episode I, C-3PO 
:::

---

In machine learning you encounter a lot of acronyms

![Zoo of acronyms](/images/Acronym%20Zoo.png)

---

![Neural Networks](/images/Network%20typology%20diagram.png)


## Look under the hood {background-color="black" background-image="/images/Starwars C-3PO 3.webp"}

::: footer
Starwars Episode I, C-3PO
:::

---

![Neuron vs. Node](/images/Neuron.png)

---

So what is Machine learning then?

It is "training" a fuction to output some specific values

$$
f(x) = y
$$

$$
f(x) = Ax^2 + Bx + C
$$

Or specifically:

$$
h_\theta (x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 +  ...\theta_n x_n 
$$
$$
h_\theta (x) = \theta^T x
$$

::: aside
Features = $x$ 

Parameters = $\Theta$
:::


---

## So how does learning work {background-color="black" background-image="/images/Chappie 4.jpg"}
What is training and how does it learn

::: footer
Chappie (2015), Chappie
:::

::: {.notes}
So how does the learning part work.
:::

---

First we need some training data.
That is data for which we have both the input as well as the result.

| x | y |
|:-:|:-:|
| 1 | 2 |
| 2 | 4 |
| 3 | 5 |
| 4 | 4 |
| 5 | 5 |

## Simple Network

```{dot}

digraph  G { 
    rankdir=LR;
    rank=same;

    compound=true

    A [label="input" shape=none]
    B [label="node" shape=circle] 
    C [label="result" shape=none]

    A -> B -> C

}

```
## Simple Network
```{dot}

digraph  G { 
    rankdir=LR;
    rank=same;

    compound=true

    A [label="x" shape=none]
    B [label="x*h " shape=circle] 
    C [label="y" shape=none]

    A -> B -> C

}

```



---

We then input a value into our fucntion.

And get a random result

> $x = 1$
> $f(1) = 10$

That result we now compart to our correct value.

> $y = 2$
> 
> $(10-2)^2 = 64$  ^[Mean Squared Error (MSE)]

---

That gives us a value for how well our model performs.

> $loss = 64$ _(Not that great)_ 

---

Net we adjust some values in our model an repeat the process ($loss$).

```{python}
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(43)

# Define the dataset
X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

# Define the model
def linear_regression(X, theta):
    return theta[0] + theta[1]*X

# Define the loss function
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

# Initialize the parameters
theta = np.random.randn(2)

# Set the learning rate
learning_rate = 0.01


fig, axs = plt.subplots(4, 4)
fig.tight_layout()

def ax_gettter():
  for list in axs:
    for item in list:
      yield item

get_ax = ax_gettter()


losses = []
thetas = []

# Train the model
for i in range(48):
    # Make a prediction
    y_pred = linear_regression(X, theta)
    
    # Calculate the loss
    loss = mse(y, y_pred)
    losses.append(loss)
    
    # Calculate the gradients
    d_theta = np.array([np.mean(y_pred - y), np.mean((y_pred - y)*X)])
    
    # Update the parameters
    theta = theta - learning_rate*d_theta
    thetas.append(theta)
    
    if i%3 == 0:
      # Plot the data and the current line of best fit
      ax = next(get_ax)

      ax.scatter(X, y)
      ax.plot(X, linear_regression(X, theta), color='r')
      ax.set_title(f'Iteration {i+1}, Loss: {loss:.2f}')

      ax.set_xlim((0,6))
      ax.set_ylim((-2,6))

      # ax.set_xticks([])
      # ax.set_yticks([])

plt.show()

```


## Simplified

So for the fucntion

$f(x) = y$

| x | y |
|:-:|:-:|
| 2 | 16|
| 3 | 24|

$f(x) = p*x$

What is $p$ ?



## Loss over $p$

```{python}

def func(x=2, p=8):
  return p*x

ps = np.arange(0,16,0.5)
erros = [ mse(func(p=8), func(p=p) ) for p in ps ]

plt.plot(ps, erros, color='r')

ps = [15,13,11.8,10,9.2,8.5,8]
erros = [mse(func(), func(p=p)) for p in ps]

plt.plot(ps, erros , color='b')
plt.scatter(ps, erros, color='b')

# for i in range(len(ps)-1):
#   x = ps[i]
#   y = erros[i]

#   dx = ps[i + 1] - x
#   dy = erros[i + 1] -y

#   # l = np.sqrt(np.sum([np.square(dx), np.square(dy)]))

#   # f = 10
#   # dx = (dx / l) * f
#   # dy = (dy / l) * f

#   plt.arrow( x, y, dx, dy, head_width=0.05, head_length=0.1, fc='k', ec='k')

plt.show()

```

$p = 8$

---

## Gradient Decent

![Gradient decent](/images/Optimisation.png)


## Loss Landscape
![Visualisation of a loss landscape](/images/Visualizing%20the%20Loss%20Landscape%20of%20Neural%20Nets.png)


## Exahution{background-image="/images/Ex Machina 4.jpg" background-color="black"}

::: footer
Ex Machina (2014), Ava
:::


# ChatGPT {background-color="black" background-image="/images/Archive 2020 -1 cropped.jpg"}

General Pretrained <u>Transformer</u> (GPT) 


::: footer
Archive (2020), George Almore & J3
:::

## 3 Key concepts

- Tokens (Words)
- Embeddign (Vector)
- Attention (Mask)

## Tokens

[![](/images/Tokenizer.png)](https://platform.openai.com/tokenizer)
<!-- Tokenizer at https://platform.openai.com/tokenizer -->

---

![](/images/Tokenizer-ids.png)

---

## Embedding {background-color="black" background-image="/images/Ghost-in-the-Shell-Trailer-Monks.jpg"}

::: footer
Ghost in the Shell (2017), Monks
:::

## {background-iframe="http://projector.tensorflow.org/"}

::: footer
http://projector.tensorflow.org/
:::
---

```{python}
import matplotlib.pyplot as plt

# Set points 
plt.scatter([2, 5],[5, 4], c="b")
plt.scatter([1, 4],[3, 2], c="r")

# Set Text labeles
plt.text(2.2,5, "Man")
plt.text(5.2,4, "Woman")
plt.text(1.2,3, "King")
plt.text(4.2,2, "Queen")

# Draw arrows
plt.arrow(2,5, 3, -1)
plt.arrow(1,3, 3, -1)

# Set dimentions
plt.xlim((0,6))
plt.ylim((0,6))

# Remove scale ticks
plt.xticks([])
plt.yticks([])

plt.show()
```

---
Crown + Person => King

```{python}
import matplotlib.pyplot as plt

person = (2,1)
crown = (2.5,4)

king = (person[0] + crown[0], person[1] + crown[1])

# Set points 
plt.scatter([person[0], crown[0]], [person[1], crown[1]], c="b")
plt.scatter(king[0], king[1], c="r")

# Set Text labeles
plt.text(person[0]+0.2,person[1], "Person")
plt.text(crown[0]+0.2,crown[1], "Crown")
plt.text(king[0]+0.2, king[1], "King")


# Draw arrows
plt.arrow(0,0, person[0], person[1], color="b")
plt.arrow(0,0, crown[0], crown[1], color="b")

plt.arrow(crown[0], crown[1], person[0], person[1], color="r")

# Set dimentions
plt.xlim((0,6))
plt.ylim((0,6))

# Remove scale ticks
plt.xticks([])
plt.yticks([])

plt.show()
```

## Transformers {background-image="/images/Terminator 2.jpg" background-color="black"}
Attention is all you need

::: footer
Terminator
:::
---

![](/images/Attention-weights-are-visualized-on-two-sentences-by-PSRTN_W640.jpg)


# Latent Diffusion{background-image="/images/Westworld 5.jpg" background-color="black"}
Stable Diffusion, Dall-E,  Midjourney

::: footer
Westworld (2016-2022), Intro
:::

---

<!-- ## {background-image="/images/image-46-58.jpg" background-color="black"} -->

![](/images/image-46-58.jpg)

Prompt:

::: {style="color:black; background-color:rgba(255,255,255, 0.5);font-size: 20px; border-radius:25px; padding:25px"}

Residential home high end futuristic interior, olson kundig::1 Interior Design by Dorothy Draper, maison de verre, axel vervoordt::2 award winning photography of an indoor-outdoor living library space, minimalist modern designs::1 high end indoor/outdoor residential living space, rendered in vray, rendered in octane, rendered in unreal engine, architectural photography, photorealism, featured in dezeen, cristobal palma::2.5 chaparral landscape outside, black surfaces/textures for furnishings in outdoor space::1 –q 2 –ar 4:7

:::


## Latent Diffusion 


```{dot}
digraph  G { 
    /* set direction of graph to be left-->right */
    rankdir=LR;
    rank=same;
    node [shape=box];
    /* make boxes instead of ellipses */
    /* should enforce nodes to be horizontally aligned */
    /* is not working, though... */
    compound=true

    /* assign labels to nodes */
    A [ label="", width=1, height=1,];
    B [ label="", width=1, height=1,];
    C [ label="", width=1, height=1,];
    D [ label="", width=1, height=1,];
    E [ label="", width=1, height=1,];

    subgraph cluster_A {
        label="t=0"
        A
    }

    subgraph cluster_B {
        label="t=1"
        B
    }

    subgraph cluster_C {
        rank=same
        label="t=2"
        C
    }

    subgraph cluster_D {
      label="t= ..."
      D
    }

    subgraph cluster_E {
      label="t=n"
      E
    }

    A -> B [label="noise", ltail=cluster_A, lhead=cluster_B];
    B -> C [label="noise", ltail=cluster_B, lhead=cluster_C];
    C -> D [label="noise", ltail=cluster_C, lhead=cluster_D];
    D -> E [label="noise", ltail=cluster_D, lhead=cluster_E];

    E:s -> D:s [label="sharpen", ltail=cluster_E, lhead=cluster_D];
    D:s -> C:s [label="sharpen", ltail=cluster_D, lhead=cluster_C];
    C:s -> B:s [label="sharpen", ltail=cluster_C, lhead=cluster_B];
    B:s -> A:s [label="sharpen", ltail=cluster_B, lhead=cluster_A];

    // Force pattern
    A -> B [label="noise", ltail=cluster_A, lhead=cluster_B, style=invis];
    B -> C [label="noise", ltail=cluster_B, lhead=cluster_C, style=invis];
    C -> D [label="noise", ltail=cluster_C, lhead=cluster_D, style=invis];
    D -> E [label="noise", ltail=cluster_D, lhead=cluster_E, style=invis];
 } 
```

## Latent Diffusion 

![](/images/Latent%20Diffusion.png)



## Auto Encoder
Or what is a le

![](/images/Auto%20Encoder.png)

## Stable Diffusion

![](/images/Stable%20Diffusuion.png)


# Applications in Practice{background-image="/images/Finch 2021.jpg" background-color="black"}

::: footer
Finch (2021), Finch Weinberg & Dewey
:::


## Render from Rhino
<!-- {background-image="/images/Ex Machina 5.jpg"} -->
[Mikkel Steenberg](https://www.linkedin.com/in/mikkelsteenberg/recent-activity/shares/)

<!-- ::: footer
Ex Machina (2014), Ava
::: -->


:::{.r-stack}

![](/images/Copenhagen%20Rhino%20View.jpg)

![](/images/Copenhagen%20Street.jpg){.fragment}

![](/images/Copenhagen%20Flooded.jpg){.fragment}

![](/images/Copenhagen%20Walking%20Street.jpg){.fragment}

:::

<!-- ::: aside
Screen shot from Rhino -> 

[Street in Copenhagen, Flooded with water, Walking Street]
::: -->


## ArkoAI {background-iframe="https://arko.ai/rhino3d"}

::: {.r-stack}
![](/images/ArkoAI%20Exterior%20Cube.png){ }

![](/images/ArkoAI%20Exterior%20Detailed.png){.fragment}

![](/images/ArkoAI%20Interior.png){.fragment}

:::


## Sketch to 3D Model
Nicholas John

<iframe src="https://dms.licdn.com/playlist/C4E05AQGGVutSJ_DiJw/mp4-720p-30fp-crf28/0/1679327160777?e=1680256800&v=beta&t=TVZ1-Fz1LeUCyreVzTDX9gbE2_ABvz99X9YEbyaOfKg" width="100%" height="600px"></iframe> 



## Hypar
<!-- ## Hypar{background-image="/images/Invasion 2020  backdrop-1920.jpg" background-color="black"}
::: footer
Invasion (2020), Julia
::: -->

![](/images/Hypar.png)

## OpenAI GPT 4

<iframe src="https://dms.licdn.com/playlist/C4E05AQGEFeIScwV4fw/mp4-720p-30fp-crf28/0/1678864869549?e=1680256800&v=beta&t=8Tn2TKuiO49ZdYti9a3FPbekDtt7gjiRlNewk_CdlR8" width="100%" height="600 px"></iframe>

<!-- 
## InFraReD{background-image="/images/blade-runner-2049-2017-lu.jpg"}

Intelligent Framework for Resilient Design
[](infrared.city)

::: footer
Blade Runner (2049), 'K'
:::

---

## Monolith AI{ visibility="uncounted" background-image="/images/BladeRunner2049-11.jpg" background-color="black"}

[Monolith AI](https://www.monolithai.com)

::: footer
Blade Runner (2049), 'K' & Rachael
:::
 -->

# My PhD {background-image="/images/War-Games-Movie-Poster-James-White-Regular-1-cropped.jpg" background-color="black"}
Promoting the reuse of vacant buildings \
using machine learing

::: footer
WarGames (1983), David
:::

## Introduction

Started : 01.12.2021

The PhD is a collaboration between:

::: {style="font-size:30px"}
* LINK Arkitektur A/S
* Aahus School of Architecture
* Multiconsult ASA \
LINK’s parent company and Norwegian engineering consortium.
* Alexandra Institute A/S \
Consultancy with a focus on helping industry adopt the newest IT standards.
* Freja Ejendomme A/S \
State owned realtor which has access to interesting properties worth exploring.
:::

## Modulsystemer i Bygningskulturarven {auto-animate=true}

:::: {.columns}

::: {.column width="40%"}
![](/images/Modulsystemer-i-bygningskulturarven.png)
:::

::: {.column width="60%" style="font-size:30px"}
Research project form 2020 at the Aarhus School of Architecture.
Involving the development of a modular system and a strategy for how to approach the transformation of this building.

 - Anne Mette Boye
 - Charlotte Bundgaard
 - Jan Buthke
 - Niels Martin Larsen
 - Simon Ostenfeld Pedersen
:::

::::

---

![Point cloud scan of Fabers Fabrikker](/images/3DScan_1.jpg)

---

Geometry extraction from point cloud

:::: {.columns style="display: flex;" }

::: {.column width="40%" style="font-size:15px" }
![](/images/Modulsystemer-i-bygningskulturarven_Rapport-20201220-screen_Page_45.jpg)
<!-- <img src="/images/Modulsystemer-i-bygningskulturarven_Rapport-20201220-screen_Page_45.jpg" /> -->

:::

::: {.column width="60%"}
![](/images/Modulsystemer-i-bygningskulturarven_Rapport-20201220-screen_Page_29-cropped.jpg)
<!-- <img src="/images/Modulsystemer-i-bygningskulturarven_Rapport-20201220-screen_Page_29-cropped.jpg"/> -->

:::

:::

---

House in house concept

:::: {.columns style="display: flex;" }

::: {.column width="40%" style="font-size:15px" }

<img height=250px src="/images/Traditionel Ombyginig i Faaborg-Midtfyn Kommune.png" />
<p>Tradition reconstruction in Faaborg-Midtfyn Konnune</p>

<img height=250px src="/images/Ombygning efter Hus i Hus Konceptet.png"/>
<p>Transformation based on the house in house concept</p>

:::

::: {.column width="60%"}
<img src="/images/Modulsystemer-i-bygningskulturarven_Rapport-20201220-screen_Page_49.jpg"/>

:::

:::


---

Result

:::: {.columns  style="display: flex;"}

::: {.column width="50%" style="height: 600px; background: url(/images/01-38749b736fc34f9a.jpg) no-repeat center center;background-size:cover; margin: 0 auto; margin-right: 5px;"}
:::


::: {.column width="50%" style="height: 600px; background: url(/images/12-86b55fddbc997e10.jpg) no-repeat center center;background-size:cover; margin: 0 auto; margin-left: 5px;"}
:::

::::

---

<!-- <img src="/images/IMG_5895.jpg" width="100" height="100" /> -->

![The project won the renovation price in 2022
](/images/IMG_5895.jpg)

## But what about me? {background-image="/images/finch3.jpg" background-color="black"}

::: footer
Finch (2021), Finch Weinberg & Dewey
:::

## {background-image="/images/Interior_pre_renovation.jpg" background-color="black"}

## {background-image="/images/Interior_post_renovation.jpg" background-color="black"}

---

![Circular economy](/images/Circular%20Economy%20Flow.png)


## Hypothesis

:::: {.columns}

::: {.column width="15%" }

<img src="/images/Shape_Complexity.png" height="80" />
<img src="/images/Shape_Time.png" height="80" />
<img src="/images/Shape_Cost.png" height="80" />
<img src="/images/Shape_History.png" height="80" />
<img src="/images/Shape_Know-How.png" height="80" />

<!-- ![](/images/Shape_Complexity.png)
![](/images/Shape_Time.png)
![](/images/Shape_Cost.png)
![](/images/Shape_History.png)
![](/images/Shape_Know-How.png) -->


:::

::: {.column width="85%" }
* High complexity in construction
* Complex planning and longer times
* Increased cost
* Historical value not quantifiable
* Practical and technological know- how

:::

:::

---

![](/images/Point%20Cloud%20Model.png)

## Process now

![](/images/Process%20Now.png)

## Process vision

![](/images/Prosess.png)

## Challanges

![](/images/Understanding%20Point%20Clouds.png)

## Example

![](/images/Segmented%20point%20cloud%20top%20view.png)

## Example

:::: {.columns style="display: flex;" }

::: {.column width="40%" style="font-size:15px" }

<img src="/images/Frequency analysis 1.png" />
<!-- <p>Tradition reconstruction in Faaborg-Midtfyn Konnune</p> -->

<img src="/images/Frequency analysis 2.png"/>
<!-- <p>Transformation based on the house in house concept</p> -->

<img src="/images/PointNet++.png"/>
<!-- <p>Transformation based on the house in house concept</p> -->

:::

::: {.column width="60%"}
<img src="/images/Exploded Room.png"/>

:::

:::

## Example

![](/images/Segmented%20floorplan.png)


## Image Kernals

![](/images/Image%20Kernal.png)

## Image Kernals

![](/images/Image%20Kernal%20MNIST.png)

## RGB-D

![](/images/NYU%20Depth%20v2.jpg)


## Workflow

![Idealised](/images/Structured%20Communication%20and%20Data%20(2).png)


## Workflow

![Reality](/images/Unstructured%20Communication%20and%20Data%20(2).png)


## Web App _(Viewer)_

![](/images/Viewer.png){.r-stretch}

## Web App _(Viewer)_

![](/images/Viewer%20with%20menu.png){.r-stretch}


## Web App _(Report)_

![](/images/Overview.png){.r-stretch}

## Web App _(Report)_

![](/images/Daylight%20Report.png){.r-stretch}

## Web App _(Report)_

![](/images/Climate%20Report.png){.r-stretch}

## Web App _(Report)_

![](/images/Historical%20Report.png){.r-stretch}



